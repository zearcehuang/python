{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import configparser\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.env')\n",
    "db_UserName = config.get('DEFAULT', 'DB_USERNAME')\n",
    "db_Password = config.get('DEFAULT', 'DB_PASSWORD')\n",
    "db_Name = config.get('DEFAULT', 'DB_NAME')\n",
    "db_Host = config.get('DEFAULT', 'DB_HOST')\n",
    "\n",
    "cnxn_str = (\"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "            f\"Server={db_Host};\"\n",
    "            f\"Database={db_Name};\"\n",
    "            f\"UID={db_UserName};\"\n",
    "            f\"PWD={db_Password};\")\n",
    "\n",
    "cnxn = pyodbc.connect(cnxn_str)\n",
    "# Create a cursor from the connection\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL查詢語句\n",
    "query = (\"select id,title ,context from (\"\n",
    "         \"select a.id,title,context from pttpost_referendum_3 a \"\n",
    "         \" inner join pttpost b on a.source=b.source and a.id=b.Id \"\n",
    "         \" where not exists (select * from keyword where source=99 and (b.title like '%'+keyname+'%' or b.context like '%'+keyname+'%')) \"\n",
    "         \" union all \"\n",
    "         \" select a.id,title,context from pttpost_referendum_3 a \"\n",
    "         \" inner join pttpostgossing b on a.source=b.source and a.id=b.Id \"\n",
    "         \" where not exists (select * from keyword where source=99 and (b.title like '%'+keyname+'%' or b.context like '%'+keyname+'%')) \"\n",
    "         \" union all \"\n",
    "         \" select convert(varchar,a.id),title,content from dcard.dbo.pttpost_referendum_3 a \"\n",
    "         \" inner join dcard.dbo.post b on a.source=b.forum and a.id=b.Id \"\n",
    "         \" where not exists (select * from keyword where source=99 and (b.title like '%'+keyname+'%' or b.content like '%'+keyname+'%')) \"\n",
    "         \" ) m \"\n",
    "         \"where 1=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HuanChen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 讀取資料表\n",
    "df = pd.read_sql(query, cnxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\project\\python\\dict.big5.txt ...\n",
      "Loading model from cache C:\\Users\\HuanChen\\AppData\\Local\\Temp\\jieba.u7bf78fb8a3e5c528afaa2a9a1de33675.cache\n",
      "Loading model cost 1.237 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 設置TF-IDF參數\n",
    "topK = 50\n",
    "withWeight = True\n",
    "\n",
    "# 載入知網詞庫\n",
    "jieba.set_dictionary('C:\\project\\python\\dict.big5.txt')\n",
    "\n",
    "# 載入自定義詞庫\n",
    "jieba.load_userdict('C:\\project\\python\\main.txt')\n",
    "\n",
    "# 設置停用詞\n",
    "jieba.analyse.set_stop_words('C:\\project\\python\\stopWord2.txt')\n",
    "\n",
    "# 讀取停用詞表\n",
    "with open('C:\\project\\python\\stopWord2.txt', 'r', encoding='utf-8') as f:\n",
    "    stop_words = f.read().split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintKeyWord(col1, col2, resultString):\n",
    " # 取出所有關鍵詞\n",
    "    print('get all key words')\n",
    "    keywords = []\n",
    "    for index, row in df.iterrows():\n",
    "        for keyword, weight in row[col1]:\n",
    "            keywords.append((keyword, weight))\n",
    "        for keyword, weight in row[col2]:\n",
    "            keywords.append((keyword, weight))\n",
    "\n",
    "    # 轉換為dataframe\n",
    "    print('轉換為dataframe')\n",
    "    keywords_df = pd.DataFrame(keywords, columns=['keyword', 'weight'])\n",
    "\n",
    "    # 合併相同的關鍵詞，計算權重總和\n",
    "    keywords_grouped = keywords_df.groupby(\n",
    "        ['keyword']).agg({'weight': 'sum'}).reset_index()\n",
    "\n",
    "    # 按權重從大到小排序\n",
    "    print('按權重從大到小排序')\n",
    "    keywords_sorted = keywords_grouped.sort_values('weight', ascending=False)\n",
    "\n",
    "    # 取出前50個關鍵詞\n",
    "    print('取出前50個關鍵詞')\n",
    "    top_keywords = keywords_sorted.head(50)['keyword'].tolist()\n",
    "\n",
    "    # 輸出結果\n",
    "    print(f'{resultString} result:')\n",
    "    print(top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(graph, weight=None, alpha=0.85, max_iter=100, tol=1e-6, weight_args=None):\n",
    "    # 初始化權重\n",
    "    if weight is None:\n",
    "        weight = uniform_weight\n",
    "    # 初始化分數\n",
    "    scores = {node: 1.0 / len(graph) for node in graph}\n",
    "    # 開始迭代\n",
    "    for _ in range(max_iter):\n",
    "        # 計算每個節點的分數\n",
    "        new_scores = {}\n",
    "        for node in graph:\n",
    "            new_score = 0.0\n",
    "            for neighbor in graph[node]:\n",
    "                weight_value = weight(node, neighbor, graph, weight_args)\n",
    "                new_score += weight_value * scores[neighbor]\n",
    "            new_scores[node] = new_score\n",
    "        # 計算調整因子\n",
    "        sum_diff = sum(abs(new_scores[node] - scores[node]) for node in graph)\n",
    "        if sum_diff < tol:\n",
    "            break\n",
    "        # 更新分數\n",
    "        for node in graph:\n",
    "            scores[node] = alpha * new_scores[node] + (1 - alpha) / len(graph)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義權重函數\n",
    "def uniform_weight(x, y, graph, weight_args):\n",
    "    return 1.0 / len(graph[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 text-rank 分析函數\n",
    "def get_keywords_textrank(content):\n",
    "    # 使用 jieba 進行斷詞\n",
    "    words = jieba.lcut(content)\n",
    "    # 去除停用詞和非中文詞\n",
    "    words = [word for word in words if word not in stop_words and re.match(\n",
    "        '^[\\u4e00-\\u9fa5]+$', word)]\n",
    "\n",
    "    # 建立關鍵詞圖\n",
    "    graph = {}\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in graph:\n",
    "            graph[words[i]] = set()\n",
    "        for j in range(i+1, len(words)):\n",
    "            if words[j] not in graph:\n",
    "                graph[words[j]] = set()\n",
    "            if j - i > 5:\n",
    "                break\n",
    "            graph[words[i]].add(words[j])\n",
    "            graph[words[j]].add(words[i])\n",
    "    # 計算關鍵詞權重\n",
    "    scores = pagerank(graph, weight=None, alpha=0.85,\n",
    "                      max_iter=100, tol=1e-6, weight_args=None)\n",
    "    # 取得前 topK 個權重最大的關鍵詞\n",
    "    tr_keywords = []\n",
    "    for word, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)[:topK]:\n",
    "        tr_keywords.append((word, score))\n",
    "    return tr_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義處理字串的函式\n",
    "def process_text(text):\n",
    "    # 使用 jieba 分詞\n",
    "    words = jieba.cut(text)\n",
    "    # 去除停用詞\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # 回傳字詞列表\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove html tag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HuanChen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HuanChen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 去除 HTML tag\n",
    "print('remove html tag')\n",
    "df['context'] = df['context'].apply(\n",
    "    lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "df['title'] = df['title'].apply(\n",
    "    lambda x: BeautifulSoup(x, \"html.parser\").get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove special word\n"
     ]
    }
   ],
   "source": [
    "# 去除特殊符号\n",
    "print('remove special word')\n",
    "df['context'] = df['context'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['title'] = df['title'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除HTML tag\n"
     ]
    }
   ],
   "source": [
    "# 去除HTML tag\n",
    "print('去除HTML tag')\n",
    "df['context'] = df['context'].apply(lambda x: re.sub(r'<[^<]+?>', '', x))\n",
    "df['title'] = df['title'].apply(lambda x: re.sub(r'<[^<]+?>', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除標點符號及數字\n"
     ]
    }
   ],
   "source": [
    "# 去除標點符號及數字\n",
    "print('去除標點符號及數字')\n",
    "df['context'] = df['context'].apply(\n",
    "    lambda x: re.sub(r'[^\\u4e00-\\u9fa5]+', '', x))\n",
    "df['title'] = df['title'].apply(lambda x: re.sub(r'[^\\u4e00-\\u9fa5]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove stop words\n"
     ]
    }
   ],
   "source": [
    "# 去除停用词\n",
    "print('remove stop words')\n",
    "df['context'] = df['context'].apply(lambda x: ' '.join(\n",
    "    [word for word in jieba.analyse.extract_tags(x, topK=topK, withWeight=False) if word not in stop_words]))\n",
    "df['title'] = df['title'].apply(lambda x: ' '.join([word for word in jieba.analyse.extract_tags(\n",
    "    x, topK=topK, withWeight=False) if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze title stop word\n"
     ]
    }
   ],
   "source": [
    "print('start analyze title stop word')\n",
    "# 使用jieba對每個記錄的title欄位進行中文斷詞\n",
    "df['title_cut'] = df['title'].apply(lambda x: ' '.join(jieba.cut(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze context  stop word\n"
     ]
    }
   ],
   "source": [
    "print('start analyze context  stop word')\n",
    "# 使用jieba對每個記錄的context欄位進行中文斷詞\n",
    "df['context_cut'] = df['context'].apply(lambda x: ' '.join(jieba.cut(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze title tf-idf\n"
     ]
    }
   ],
   "source": [
    "print('start analyze title tf-idf')\n",
    "# 使用tf-idf算法計算每個記錄的title欄位的關鍵詞\n",
    "df['title_keywords'] = df['title_cut'].apply(\n",
    "    lambda x: jieba.analyse.extract_tags(x, topK=topK, withWeight=withWeight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze context tf-idf\n"
     ]
    }
   ],
   "source": [
    "print('start analyze context tf-idf')\n",
    "# 使用tf-idf算法計算每個記錄的context欄位的關鍵詞\n",
    "df['context_keywords'] = df['context_cut'].apply(\n",
    "    lambda x: jieba.analyse.extract_tags(x, topK=topK, withWeight=withWeight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get all key words\n",
      "轉換為dataframe\n",
      "按權重從大到小排序\n",
      "取出前50個關鍵詞\n",
      "tf-idf result:\n",
      "['問卦', '新聞', '台灣', '中國', '八卦', '爆卦', '公投', '核四', '蔡英文', '停電', '黑特', '美國', '總統', '地震', '發文', '相關', '個月', '也算', '前請', '柯文哲', '台灣人', '政治', '台電', '塔綠班', '缺電', '國家', '對岸', '網軍', '四個', '禁止', '不同意', '媒體', '發電', '高雄', '鄉民', '核廢料', '請注意', '藻礁', '記者', '中共', '萊豬', '保護', '重啟', '反核', '選舉', '中國人', '台北', '政府', '投票', '環境']\n"
     ]
    }
   ],
   "source": [
    "PrintKeyWord('title_keywords', 'context_keywords','tf-idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze context textrank\n"
     ]
    }
   ],
   "source": [
    "# 將 text-rank 分析結果加入 DataFrame 中\n",
    "print('start analyze context textrank')\n",
    "# df['tr_content_keywords'] = df['context_cut'].apply(get_keywords_textrank)\n",
    "df['tr_content_keywords'] = df['context_cut'].apply(lambda x: jieba.analyse.textrank(\n",
    "    x, topK=topK, withWeight=withWeight, allowPOS=('ns', 'n', 'vn', 'v')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze title textrank\n"
     ]
    }
   ],
   "source": [
    "print('start analyze title textrank')\n",
    "# df['tr_title_keywords'] = df['title_cut'].apply(get_keywords_textrank)\n",
    "df['tr_title_keywords'] = df['title_cut'].apply(\n",
    "    lambda x: jieba.analyse.textrank(x, topK=topK, withWeight=withWeight, allowPOS=('ns', 'n', 'vn', 'v')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get all key words\n",
      "轉換為dataframe\n",
      "按權重從大到小排序\n",
      "取出前50個關鍵詞\n",
      "text-rank result:\n",
      "['問卦', '八卦', '政治', '禁止', '政府', '公投', '好像', '台北', '日本', '不用', '支持', '朋友', '人民', '地震', '民主', '投票', '影片', '事情', '文章', '地方', '核能', '事件', '小孩', '希望', '世界', '垃圾', '小弟', '公司', '同路人', '代表', '分享', '安安', '台北市', '能源', '爆卦', '原本', '民意', '不到', '工作', '政策', '留言', '道歉', '想到', '方式', '抹黑', '肥宅', '缺水', '原因', '理由', '蟑螂']\n"
     ]
    }
   ],
   "source": [
    "PrintKeyWord('tr_content_keywords', 'tr_title_keywords','text-rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze title tf\n",
      " \n",
      "問卦\n",
      "台灣\n",
      "新聞\n",
      "八卦\n",
      "中國\n",
      "政治\n",
      "發文\n",
      "相關\n",
      "個月\n",
      "也算\n",
      "禁止\n",
      "前請\n",
      "美國\n",
      "記者\n",
      "媒體\n",
      "公投\n",
      "總統\n",
      "完整\n",
      "政府\n",
      "核四\n",
      "國家\n",
      "蔡英文\n",
      "台灣人\n",
      "新聞標題\n",
      "請注意\n",
      "報導\n",
      "中共\n",
      "停電\n",
      "鄉民\n",
      "對岸\n",
      "東西\n",
      "台電\n",
      "爆卦\n",
      "日本\n",
      "環境\n",
      "好像\n",
      "柯文哲\n",
      "台北\n",
      "選舉\n",
      "保護\n",
      "也有\n",
      "支持\n",
      "塔綠班\n",
      "不用\n",
      "發電\n",
      "社會\n",
      "地震\n",
      "不同意\n",
      "自己的\n",
      "民眾\n"
     ]
    }
   ],
   "source": [
    "print('start analyze title tf')\n",
    "# 處理 context 欄位\n",
    "corpus_context = [process_text(text) for text in df['context']]\n",
    "# 處理 title 欄位\n",
    "corpus_title = [process_text(text) for text in df['title']]\n",
    "# 合併兩個 corpus\n",
    "corpus = corpus_context + corpus_title\n",
    "\n",
    "# 使用 nltk.FreqDist 計算詞頻\n",
    "word_freq = nltk.FreqDist(word for words in corpus for word in words)\n",
    "# 取出前 50 筆(去掉空格多取一筆)\n",
    "top_words = word_freq.most_common(51)\n",
    "\n",
    "# 顯示結果\n",
    "for word, freq in top_words:\n",
    "    # print(f\"{word}: {freq}\")\n",
    "    print(f\"{word}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f2067f0617624634ae04efdae08bb6bc0d88c75cbfebf3cc8de427f8cb78762"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
