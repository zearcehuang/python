{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import configparser\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.env')\n",
    "db_UserName = config.get('DEFAULT', 'DB_USERNAME')\n",
    "db_Password = config.get('DEFAULT', 'DB_PASSWORD')\n",
    "db_Name = config.get('DEFAULT', 'DB_NAME')\n",
    "db_Host = config.get('DEFAULT', 'DB_HOST')\n",
    "\n",
    "cnxn_str = (\"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "            f\"Server={db_Host};\"\n",
    "            f\"Database={db_Name};\"\n",
    "            f\"UID={db_UserName};\"\n",
    "            f\"PWD={db_Password};\")\n",
    "\n",
    "cnxn = pyodbc.connect(cnxn_str)\n",
    "# Create a cursor from the connection\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL查詢語句\n",
    "query = (\"select id,title ,context from (\"\n",
    "         \"select a.id,title,context from pttpost_referendum_1 a \"\n",
    "         \" inner join pttpost b on a.source=b.source and a.id=b.Id \"\n",
    "         \" where not exists (select * from keyword where source=99 and (b.title like '%'+keyname+'%' or b.context like '%'+keyname+'%')) \"\n",
    "         \" union all \"\n",
    "         \" select a.id,title,context from pttpost_referendum_1 a \"\n",
    "         \" inner join pttpostgossing b on a.source=b.source and a.id=b.Id \"\n",
    "         \" where not exists (select * from keyword where source=99 and (b.title like '%'+keyname+'%' or b.context like '%'+keyname+'%')) \"\n",
    "         \" union all \"\n",
    "         \" select convert(varchar,a.id),title,content from dcard.dbo.pttpost_referendum_1 a \"\n",
    "         \" inner join dcard.dbo.post b on a.source=b.forum and a.id=b.Id \"\n",
    "         \" where not exists (select * from keyword where source=99 and (b.title like '%'+keyname+'%' or b.content like '%'+keyname+'%')) \"\n",
    "         \" ) m \"\n",
    "         \"where 1=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HuanChen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 讀取資料表\n",
    "df = pd.read_sql(query, cnxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\project\\python\\dict.big5.txt ...\n",
      "Loading model from cache C:\\Users\\HuanChen\\AppData\\Local\\Temp\\jieba.u7bf78fb8a3e5c528afaa2a9a1de33675.cache\n",
      "Loading model cost 1.478 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 設置TF-IDF參數\n",
    "topK = 50\n",
    "withWeight = True\n",
    "\n",
    "# 載入知網詞庫\n",
    "jieba.set_dictionary('C:\\project\\python\\dict.big5.txt')\n",
    "\n",
    "# 載入自定義詞庫\n",
    "jieba.load_userdict('C:\\project\\python\\main.txt')\n",
    "\n",
    "# 設置停用詞\n",
    "jieba.analyse.set_stop_words('C:\\project\\python\\stopWord2.txt')\n",
    "\n",
    "# 讀取停用詞表\n",
    "with open('C:\\project\\python\\stopWord2.txt', 'r', encoding='utf-8') as f:\n",
    "    stop_words = f.read().split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintKeyWord(col1, col2, resultString):\n",
    " # 取出所有關鍵詞\n",
    "    print('get all key words')\n",
    "    keywords = []\n",
    "    for index, row in df.iterrows():\n",
    "        for keyword, weight in row[col1]:\n",
    "            keywords.append((keyword, weight))\n",
    "        for keyword, weight in row[col2]:\n",
    "            keywords.append((keyword, weight))\n",
    "\n",
    "    # 轉換為dataframe\n",
    "    print('轉換為dataframe')\n",
    "    keywords_df = pd.DataFrame(keywords, columns=['keyword', 'weight'])\n",
    "\n",
    "    # 合併相同的關鍵詞，計算權重總和\n",
    "    keywords_grouped = keywords_df.groupby(\n",
    "        ['keyword']).agg({'weight': 'sum'}).reset_index()\n",
    "\n",
    "    # 按權重從大到小排序\n",
    "    print('按權重從大到小排序')\n",
    "    keywords_sorted = keywords_grouped.sort_values('weight', ascending=False)\n",
    "\n",
    "    # 取出前50個關鍵詞\n",
    "    print('取出前50個關鍵詞')\n",
    "    top_keywords = keywords_sorted.head(50)['keyword'].tolist()\n",
    "\n",
    "    # 輸出結果\n",
    "    print(f'{resultString} result:')\n",
    "    print(top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(graph, weight=None, alpha=0.85, max_iter=100, tol=1e-6, weight_args=None):\n",
    "    # 初始化權重\n",
    "    if weight is None:\n",
    "        weight = uniform_weight\n",
    "    # 初始化分數\n",
    "    scores = {node: 1.0 / len(graph) for node in graph}\n",
    "    # 開始迭代\n",
    "    for _ in range(max_iter):\n",
    "        # 計算每個節點的分數\n",
    "        new_scores = {}\n",
    "        for node in graph:\n",
    "            new_score = 0.0\n",
    "            for neighbor in graph[node]:\n",
    "                weight_value = weight(node, neighbor, graph, weight_args)\n",
    "                new_score += weight_value * scores[neighbor]\n",
    "            new_scores[node] = new_score\n",
    "        # 計算調整因子\n",
    "        sum_diff = sum(abs(new_scores[node] - scores[node]) for node in graph)\n",
    "        if sum_diff < tol:\n",
    "            break\n",
    "        # 更新分數\n",
    "        for node in graph:\n",
    "            scores[node] = alpha * new_scores[node] + (1 - alpha) / len(graph)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義權重函數\n",
    "def uniform_weight(x, y, graph, weight_args):\n",
    "    return 1.0 / len(graph[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 text-rank 分析函數\n",
    "def get_keywords_textrank(content):\n",
    "    # 使用 jieba 進行斷詞\n",
    "    words = jieba.lcut(content)\n",
    "    # 去除停用詞和非中文詞\n",
    "    words = [word for word in words if word not in stop_words and re.match(\n",
    "        '^[\\u4e00-\\u9fa5]+$', word)]\n",
    "\n",
    "    # 建立關鍵詞圖\n",
    "    graph = {}\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in graph:\n",
    "            graph[words[i]] = set()\n",
    "        for j in range(i+1, len(words)):\n",
    "            if words[j] not in graph:\n",
    "                graph[words[j]] = set()\n",
    "            if j - i > 5:\n",
    "                break\n",
    "            graph[words[i]].add(words[j])\n",
    "            graph[words[j]].add(words[i])\n",
    "    # 計算關鍵詞權重\n",
    "    scores = pagerank(graph, weight=None, alpha=0.85,\n",
    "                      max_iter=100, tol=1e-6, weight_args=None)\n",
    "    # 取得前 topK 個權重最大的關鍵詞\n",
    "    tr_keywords = []\n",
    "    for word, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)[:topK]:\n",
    "        tr_keywords.append((word, score))\n",
    "    return tr_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義處理字串的函式\n",
    "def process_text(text):\n",
    "    # 使用 jieba 分詞\n",
    "    words = jieba.cut(text)\n",
    "    # 去除停用詞\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # 回傳字詞列表\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove html tag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HuanChen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HuanChen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 去除 HTML tag\n",
    "print('remove html tag')\n",
    "df['context'] = df['context'].apply(\n",
    "    lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "df['title'] = df['title'].apply(\n",
    "    lambda x: BeautifulSoup(x, \"html.parser\").get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove special word\n"
     ]
    }
   ],
   "source": [
    "# 去除特殊符号\n",
    "print('remove special word')\n",
    "df['context'] = df['context'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['title'] = df['title'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除HTML tag\n"
     ]
    }
   ],
   "source": [
    "# 去除HTML tag\n",
    "print('去除HTML tag')\n",
    "df['context'] = df['context'].apply(lambda x: re.sub(r'<[^<]+?>', '', x))\n",
    "df['title'] = df['title'].apply(lambda x: re.sub(r'<[^<]+?>', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除標點符號及數字\n"
     ]
    }
   ],
   "source": [
    "# 去除標點符號及數字\n",
    "print('去除標點符號及數字')\n",
    "df['context'] = df['context'].apply(\n",
    "    lambda x: re.sub(r'[^\\u4e00-\\u9fa5]+', '', x))\n",
    "df['title'] = df['title'].apply(lambda x: re.sub(r'[^\\u4e00-\\u9fa5]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove stop words\n"
     ]
    }
   ],
   "source": [
    "# 去除停用词\n",
    "print('remove stop words')\n",
    "df['context'] = df['context'].apply(lambda x: ' '.join(\n",
    "    [word for word in jieba.analyse.extract_tags(x, topK=topK, withWeight=False) if word not in stop_words]))\n",
    "df['title'] = df['title'].apply(lambda x: ' '.join([word for word in jieba.analyse.extract_tags(\n",
    "    x, topK=topK, withWeight=False) if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze title stop word\n"
     ]
    }
   ],
   "source": [
    "print('start analyze title stop word')\n",
    "# 使用jieba對每個記錄的title欄位進行中文斷詞\n",
    "df['title_cut'] = df['title'].apply(lambda x: ' '.join(jieba.cut(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze context  stop word\n"
     ]
    }
   ],
   "source": [
    "print('start analyze context  stop word')\n",
    "# 使用jieba對每個記錄的context欄位進行中文斷詞\n",
    "df['context_cut'] = df['context'].apply(lambda x: ' '.join(jieba.cut(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze title tf-idf\n"
     ]
    }
   ],
   "source": [
    "print('start analyze title tf-idf')\n",
    "# 使用tf-idf算法計算每個記錄的title欄位的關鍵詞\n",
    "df['title_keywords'] = df['title_cut'].apply(\n",
    "    lambda x: jieba.analyse.extract_tags(x, topK=topK, withWeight=withWeight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze context tf-idf\n"
     ]
    }
   ],
   "source": [
    "print('start analyze context tf-idf')\n",
    "# 使用tf-idf算法計算每個記錄的context欄位的關鍵詞\n",
    "df['context_keywords'] = df['context_cut'].apply(\n",
    "    lambda x: jieba.analyse.extract_tags(x, topK=topK, withWeight=withWeight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintKeyWord('title_keywords', 'context_keywords','tf-idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze context textrank\n"
     ]
    }
   ],
   "source": [
    "# 將 text-rank 分析結果加入 DataFrame 中\n",
    "print('start analyze context textrank')\n",
    "df['tr_content_keywords'] = df['context_cut'].apply(get_keywords_textrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze title textrank\n"
     ]
    }
   ],
   "source": [
    "print('start analyze title textrank')\n",
    "df['tr_title_keywords'] = df['title_cut'].apply(get_keywords_textrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintKeyWord('tr_content_keywords', 'tr_title_keywords','text-rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start analyze title tf\n",
      " : 7005393\n",
      "台灣: 57209\n",
      "民進黨: 27130\n",
      "完整: 22545\n",
      "媒體: 21668\n",
      "中國: 20582\n",
      "國民黨: 19101\n",
      "有沒有: 18556\n",
      "都是: 16376\n",
      "美國: 14819\n",
      "立委: 13498\n",
      "蔡英文: 13459\n",
      "公投: 13252\n",
      "真的: 11766\n",
      "總統: 10947\n",
      "的人: 10924\n",
      "政府: 10612\n",
      "不知道: 9841\n",
      "柯文哲: 9672\n",
      "國家: 9161\n",
      "發現: 9021\n",
      "核四: 8763\n",
      "民眾: 8627\n",
      "時間: 8450\n",
      "台北: 8365\n",
      "台灣人: 8346\n",
      "的時候: 8088\n",
      "一堆: 8042\n",
      "桃園: 7325\n",
      "內容: 7242\n",
      "引述: 7188\n",
      "如題: 7176\n",
      "備註: 7091\n",
      "發生: 7067\n",
      "市長: 6951\n",
      "這是: 6894\n",
      "高雄: 6697\n",
      "感覺: 6689\n",
      "臉書: 6580\n",
      "的是: 6536\n",
      "選舉: 6375\n",
      "中共: 6234\n",
      "社會: 6226\n",
      "未來: 6176\n",
      "網址: 6164\n",
      "罷免: 6031\n",
      "議員: 5946\n",
      "刪除: 5815\n",
      "請問: 5624\n",
      "東西: 5527\n"
     ]
    }
   ],
   "source": [
    "print('start analyze title tf')\n",
    "# 處理 context 欄位\n",
    "corpus_context = [process_text(text) for text in df['context']]\n",
    "# 處理 title 欄位\n",
    "corpus_title = [process_text(text) for text in df['title']]\n",
    "# 合併兩個 corpus\n",
    "corpus = corpus_context + corpus_title\n",
    "\n",
    "# 使用 nltk.FreqDist 計算詞頻\n",
    "word_freq = nltk.FreqDist(word for words in corpus for word in words)\n",
    "# 取出前 50 筆\n",
    "top_words = word_freq.most_common(50)\n",
    "\n",
    "# 顯示結果\n",
    "for word, freq in top_words:\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f2067f0617624634ae04efdae08bb6bc0d88c75cbfebf3cc8de427f8cb78762"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
